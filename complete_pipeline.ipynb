{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b4da00",
   "metadata": {},
   "source": [
    "**Author :** Lorenzo Bortolussi (Bachelor Thesis), academic year 2024/2025\n",
    "\n",
    "# Contextual Preference Ranking for Card Recommendations in Magic: The Gathering\n",
    "This notebook demonstrates the complete, end-to-end pipeline for the Card Recommender System developed for this thesis. It covers every stage of the project, from initial data acquisition to the final training and deployment of the models.\n",
    "\n",
    "The pipeline is divided into seven distinct stages:\n",
    "1.  **Corpus Creation:** Scraping and assembling a diverse text corpus for domain adaptation of an LLM.\n",
    "2.  **Language Model Training:** Fine-tuning the foundation and classifier models.\n",
    "3.  **Card Data Retrieval & Representation:** Downloading, filtering, and vectorizing all card data.\n",
    "4.  **Decklist & Dataset Creation:** Scraping decklists and creating the final training datasets.\n",
    "5.  **CPR Model Training:** Training the core recommender models with various configurations.\n",
    "6.  **Vector Database Construction:** Populating the ChromaDB collections for efficient search.\n",
    "7.  **Gradio Demo Launch:** Running the interactive web application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa709ae",
   "metadata": {},
   "source": [
    "## Stage 1: Corpus Creation for Domain Adaptation\n",
    "The first step is to create a rich, domain-specific text corpus. This involves scraping thousands of paragraphs from online articles related to *Magic: The Gathering* strategy. This colloquial text is essential for teaching a language model how players actually talk about the game. This data will be combined with official rules and card texts in the next stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WARNING --- \n",
    "# This is a long-running process that scrapes hundreds of web pages.\n",
    "# It is designed to be restartable. If it stops, run it again to resume.\n",
    "\n",
    "import article_scraper\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "output_file = str(base_dir / \"data\" / \"scraped_articles.txt\")\n",
    "\n",
    "articles_scraper = article_scraper.ArticlesScraper(\n",
    "    stop_phrases=article_scraper.Params.stop_phrases, \n",
    "    lang_check=article_scraper.Params.lang_check,\n",
    "    requests_per_second=article_scraper.Params.requests_per_second\n",
    ")\n",
    "\n",
    "articles_scraper.scrape_articles_into_paragraphs(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e6b79",
   "metadata": {},
   "source": [
    "## Stage 2: Card Data Retrieval\n",
    "Next, we download the latest card data from Scryfall, apply filters to create a clean dataset of Commander-legal cards, and download their images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WARNING ---\n",
    "# This stage downloads a large JSON file and potentially thousands of images.\n",
    "\n",
    "import utils\n",
    "import preprocess_cards\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "data_dir = base_dir / \"data\"\n",
    "this = str(Path().resolve()) # Emulate os.path.dirname(__file__)\n",
    "raw_data = str(data_dir / \"raw_data.json\")\n",
    "clean_data = str(data_dir / \"clean_data.json\")\n",
    "img_dir = str(data_dir / \"images\")\n",
    "\n",
    "download = input(\"Download fresher data? (Y/N): \").strip().lower() == \"y\"\n",
    "if download:\n",
    "    preprocess_cards.download_data(raw_data)\n",
    "else:\n",
    "    print(\"Download skipped.\")\n",
    "    \n",
    "preprocess_cards.filter_data(raw_data, clean_data)\n",
    "preprocess_cards.download_images(clean_data, output_folder=img_dir)\n",
    "\n",
    "utils.generate_and_save_dict(this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4db283",
   "metadata": {},
   "source": [
    "## Stage 3: Language Model Training\n",
    "In order to build the the multi-modal vector representations for the cards, we first need to fine tune a pretrained language model in order to process the text of the cards appropriately.\n",
    "\n",
    "Using the corpus assembled on **Stage 1**, we execute the three-stage pipeline to train two expert language models:\n",
    "1.  **Foundation Model:** A `distilbert-base-uncased` model is fine-tuned on the composite corpus using Masked Language Modeling.\n",
    "2.  **Pseudo-Labeling:** A zero-shot NLI model generates role labels for all cards.\n",
    "3.  **Classifier Model:** The foundation model is further fine-tuned on the pseudo-labels to become an expert card role classifier.\n",
    "\n",
    "In order to complete the text corpus for this training, please download the *Magic: The Gathering* complete rulebook from [this link](https://media.wizards.com/2025/downloads/MagicCompRules%2020250919.txt) and place it into the data folder as **rules.txt**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7370e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WARNING ---\n",
    "# This is a very long-running, GPU-intensive process.\n",
    "# The script will automatically use a CUDA-enabled GPU if available.\n",
    "\n",
    "import text_embeddings\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "data_dir = base_dir / \"data\"\n",
    "models_dir = base_dir / \"models\"\n",
    "text_embeddings.Paths.clean_data_json = str(data_dir / \"clean_data.json\")\n",
    "text_embeddings.Paths.rules_txt =  str(data_dir / \"mtg_rules.txt\")\n",
    "text_embeddings.Paths.fundation_model =  str(models_dir / \"magic-distilbert-base-v1\")\n",
    "text_embeddings.Paths.pseudo_labeled_dataset = str(data_dir / \"card_roles_dataset.jsonl\")\n",
    "text_embeddings.Paths.final_classifier = str(models_dir / \"card-role-classifier-final\")\n",
    "text_embeddings.Paths.scraped_articles_txt = str(data_dir / \"scraped_articles.txt\")\n",
    "\n",
    "# Call the three stage pipeline\n",
    "text_embeddings.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86435056",
   "metadata": {},
   "source": [
    "## Stage 4: Card Representations\n",
    "We can now process the clean data from **stage 2** to create the multi-modal vector representations for every card, which are the fundamental building blocks for our recommender model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcad48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_cards.build_card_representations(this, batch_size=16, use_img=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c0d7c",
   "metadata": {},
   "source": [
    "## Stage 5: Decklist & Dataset Creation\n",
    "This stage scrapes thousands of Commander decklists from Archidekt. The raw decklists are then processed into two distinct training datasets (`_all` and `_div`) that the CPR model will be trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180beb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---  WARNING --- \n",
    "# Scraping 100,000 decks can take over 15 hours. This process can be halted but is not restartable.\n",
    "# It is highly recommended to run this once and save the output files.\n",
    "\n",
    "import edh_scraper\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "data_dir = base_dir / \"data\"\n",
    "card_dict_path = str(data_dir / \"card_dict.pt\")\n",
    "decks_path_all = str(data_dir / \"edh_decks_all.jsonl\")\n",
    "decks_path_div = str(data_dir / \"edh_decks_div.jsonl\")\n",
    "\n",
    "dataset_path_all = str(data_dir / \"cpr_dataset_v1_all.pt\")\n",
    "dataset_path_div = str(data_dir / \"cpr_dataset_v1_div.pt\")\n",
    "card_feat_map_path = str(data_dir / \"card_repr_dict_v1.pt\")\n",
    "cat_feat_map_path = str(data_dir / \"type_and_keyw_dict.pt\")\n",
    "\n",
    "edh_scraper.main(\n",
    "    this=this,\n",
    "    card_dict=card_dict_path,\n",
    "    out_jsonl=decks_path_all,\n",
    "    out_jsonl_diversified=decks_path_div,\n",
    "    max_archidekt=100000,\n",
    "    per_color_bucket=3000,\n",
    "    n_duplicates_per_strategy = 10,\n",
    "    rate_per_sec=4.0\n",
    ")\n",
    "\n",
    "# --- Create Datasets from Scraped Decklists ---\n",
    "# This part is much faster and can be run independently, given the .jsonl files are present.\n",
    "create_and_save_CPRdataset(decks_path_div, dataset_path_div, card_feat_map_path, cat_feat_map_path)\n",
    "create_and_save_CPRdataset(decks_path_all, dataset_path_all, card_feat_map_path, cat_feat_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cd020",
   "metadata": {},
   "source": [
    "## Stage 6: CPR Model Training\n",
    "This is the core training stage. Eight different versions of the CPR recommender model are trained, varying the dataset, loss function, and number of epochs. After each model is trained, they are used to generate a corresponding dictionary of card embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1018be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WARNING ---\n",
    "# This is a long-running, GPU-intensive process that trains 8 separate models.\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import train\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "data_dir = base_dir / \"data\"\n",
    "models_dir = base_dir / \"models\"\n",
    "\n",
    "dataset_path_all = str(data_dir / \"cpr_dataset_v1_all.pt\")\n",
    "dataset_path_div = str(data_dir / \"cpr_dataset_v1_div.pt\")\n",
    "cat_feat_map_path = str(data_dir / \"type_and_keyw_dict.pt\")\n",
    "card_feat_map_path = str(data_dir / \"card_repr_dict_v1.pt\")\n",
    "\n",
    "# Determine number of types/keywords from data\n",
    "types_and_keyw_dict = torch.load(cat_feat_map_path, map_location=\"cpu\")\n",
    "first_entry = next(iter(types_and_keyw_dict.values()))\n",
    "num_types = len(first_entry['types'])\n",
    "num_keyw = len(first_entry['keywords'])\n",
    "\n",
    "# --- Training Loop for all configurations ---\n",
    "for ds_path, ds_name in [(dataset_path_all, \"all\"), (dataset_path_div, \"div\")]:\n",
    "    for epochs in [20, 200]:\n",
    "        # Train with Triplet Loss\n",
    "        triplet_checkpoint_path = str(models_dir / f\"cpr_checkpoint_v1_{ds_name}_{epochs}_3.pt\")\n",
    "        train.main_cpr_training(\n",
    "            cpr_dataset_path=ds_path,\n",
    "            cpr_checkpoint_path=triplet_checkpoint_path,\n",
    "            loss_fn=nn.TripletMarginLoss(margin=0.3),\n",
    "            step_fn=train.cpr_step_fn_triplet,\n",
    "            NUM_EPOCHS=epochs,\n",
    "            NUM_TYPES=num_types, NUM_KEYW=num_keyw\n",
    "        )\n",
    "\n",
    "        # Train with InfoNCE Loss\n",
    "        infonce_checkpoint_path = str(models_dir / f\"cpr_checkpoint_v1_{ds_name}_{epochs}_nce.pt\")\n",
    "        train.main_cpr_training(\n",
    "            cpr_dataset_path=ds_path,\n",
    "            cpr_checkpoint_path=infonce_checkpoint_path,\n",
    "            loss_fn=nn.CrossEntropyLoss(),\n",
    "            step_fn=train.cpr_step_fn_infonce,\n",
    "            NUM_EPOCHS=epochs,\n",
    "            NUM_TYPES=num_types, NUM_KEYW=num_keyw\n",
    "        )\n",
    "\n",
    "# --- Generate Embedding Dictionaries for all trained models ---\n",
    "for ds_name in [\"all\", \"div\"]:\n",
    "    for epochs in [\"20\", \"200\"]:\n",
    "        for loss_name in [\"3\", \"nce\"]:\n",
    "            emb_dict_path = str(data_dir / f\"emb_dict_v1_{ds_name}_{epochs}_{loss_name}.pt\")\n",
    "            cpr_checkpoint_path = str(models_dir / f\"cpr_checkpoint_v1_{ds_name}_{epochs}_{loss_name}.pt\")\n",
    "            train.generate_and_save_emb_dict(\n",
    "                card_feat_map_path, cat_feat_map_path, cpr_checkpoint_path,\n",
    "                num_types, num_keyw, batch_size=64, out_path=emb_dict_path\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f09b8e",
   "metadata": {},
   "source": [
    "## Stage 7: Vector Database Construction\n",
    "With the embedding dictionaries created for all eight models, we now populate the ChromaDB vector database. A separate collection is created for each model, allowing the final application to query them independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8351cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vector_database\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "data_dir = base_dir / \"data\"\n",
    "db_path = str(base_dir / \"card_db\")\n",
    "client = chromadb.PersistentClient(path=db_path)\n",
    "clean_data_path = str(data_dir / \"clean_data.json\")\n",
    "\n",
    "for ds_name in [\"all\", \"div\"]:\n",
    "    for epochs in [\"20\", \"200\"]:\n",
    "        for loss_abr in [\"nce\", \"3\"]:\n",
    "            emb_dict_path = str(data_dir / f\"emb_dict_v1_{ds_name}_{epochs}_{loss_abr}.pt\")\n",
    "            db_name = f\"mtg_cards_v1_{ds_name}_{epochs}_{loss_abr}\"\n",
    "            vector_database.build_and_save_chroma_db(emb_dict_path, clean_data_path, client, db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950ccfc",
   "metadata": {},
   "source": [
    "## Stage 8: Gradio Demo Launch\n",
    "The final stage is to launch the interactive Gradio web application. This app provides an interface for querying the recommender system and collecting the valuable human feedback needed for model evaluation.\n",
    "\n",
    "**Note:** Running a Gradio app within a Jupyter Notebook can sometimes be unstable or consume excessive memory, potentially causing the kernel to crash. It is highly recommended to run the demo by executing `python app.py` from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67f5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading retriever for 'Diversified Dataset (20 Epochs, Triplet)' for the first time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name c:\\A\\UNIVERSITA'\\AIDA\\III ANNO\\thesis\\Card Recommender System\\models\\magic-distilbert-base-v1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete.\n",
      "Loading retriever for 'Complete Dataset (200 Epochs, InfoNCE)' for the first time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name c:\\A\\UNIVERSITA'\\AIDA\\III ANNO\\thesis\\Card Recommender System\\models\\magic-distilbert-base-v1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete.\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Deck ID: 9151052, Model: Diversified Dataset (20 Epochs, Triplet), Prompt: ''\n",
      "Using cached retriever for 'Diversified Dataset (20 Epochs, Triplet)'\n",
      "--- Generating synergy recommendations ---\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import app\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "models_dir = base_dir / \"models\"\n",
    "\n",
    "# For this example, only two models are made available\n",
    "app.model_versions = {} \n",
    "app.model_versions = {\n",
    "    \"Diversified Dataset (20 Epochs, Triplet)\": {\n",
    "        \"checkpoint\": str(models_dir / \"cpr_checkpoint_v1_div_20_triplet_s2.pt\"), # _s2.pt\n",
    "        \"db_name\": \"mtg_cards_v1_div_20_triplet_s2\"\n",
    "    },\n",
    "    \"Complete Dataset (200 Epochs, InfoNCE)\": {\n",
    "        \"checkpoint\": str(models_dir / \"cpr_checkpoint_v1_all_200_nce_s2.pt\"),\n",
    "        \"db_name\": \"mtg_cards_v1_all_200_nce_s2\"\n",
    "    }\n",
    "}\n",
    " \n",
    "for model_version in list(app.model_versions.keys()):\n",
    "    app.get_retriever(model_version)\n",
    "\n",
    "app.demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
